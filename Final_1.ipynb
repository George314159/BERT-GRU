{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"1429_1.csv\")\n",
    "data.columns = ['id', 'name', 'asins', 'brand', 'categories', 'keys', 'manufacturer',\n",
    "       'reviews_date', 'reviews_dateAdded', 'reviews_dateSeen',\n",
    "       'reviews_didPurchase', 'reviews_doRecommend', 'reviews_id',\n",
    "       'reviews_numHelpful', 'reviews_rating', 'reviews_sourceURLs',\n",
    "       'reviews_text', 'reviews_title', 'reviews_userCity',\n",
    "       'reviews_userProvince', 'reviews_username']\n",
    "data = data.dropna(subset=[\"reviews_text\"])\n",
    "reviews = data[\"reviews_text\"].tolist()\n",
    "reviews = [i.lower() for i in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 01:00:43.796275 4397680064 file_utils.py:39] PyTorch version 1.3.0.post2 available.\n",
      "I1212 01:00:48.509127 4397680064 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/xianglu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# BERTokenizer to transform the reivews\n",
    "from transformers import BertTokenizer\n",
    "BT = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "newData = []\n",
    "for rev in reviews:\n",
    "    newData.append(BT.convert_tokens_to_ids(rev.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding function to make the tokenizer same length\n",
    "def padding(text, length):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for review in text:\n",
    "        tlength = len(review)\n",
    "        diff = length - tlength\n",
    "        if diff == 0.0:\n",
    "            res.append(review)\n",
    "        if diff < 0.0:\n",
    "            res.append(review[:length])\n",
    "        if diff > 0.0:\n",
    "            pad = list(np.zeros(diff))\n",
    "            res.append(pad + review)\n",
    "        \n",
    "    res = np.asarray(res,dtype=int)    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the sentiment for review ratings.\n",
    "def sentiment_generation(rating):\n",
    "    if rating>4.0:\n",
    "        return \"Positive\"\n",
    "    elif rating<3.0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "data[\"Sentiment_generated\"] = data.apply(lambda x: sentiment_generation(x.reviews_rating), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "features2 = padding(newData, 30) \n",
    "\n",
    "## Sentiment label\n",
    "labels = data[\"Sentiment_generated\"].tolist() \n",
    "\n",
    "## Encode label\n",
    "encoded_labels = [1.0 if label =='Positive' else 0.0 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training, validation and testing set\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features2, encoded_labels, test_size=0.2, random_state=1)\n",
    "test_x, val_x, test_y, val_y = train_test_split(test_x, test_y,test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training x:  27727\n",
      "Length of training y:  27727\n",
      "Length of testing x:  3466\n",
      "Length of testing y:  3466\n",
      "Length of eval x:  3466\n",
      "Length of eval y:  3466\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of training x: \", len(train_x))\n",
    "print(\"Length of training y: \", len(train_y))\n",
    "print(\"Length of testing x: \", len(test_x))\n",
    "print(\"Length of testing y: \", len(test_y))\n",
    "print(\"Length of eval x: \", len(val_x))\n",
    "print(\"Length of eval y: \", len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=30)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=30)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=30)\n",
    "\n",
    "## Full DataSet\n",
    "train_dataiter = iter(train_loader)\n",
    "test_dataiter = iter(test_loader)\n",
    "validate_dataiter = iter(valid_loader)\n",
    "\n",
    "## Smaller DataSet\n",
    "\n",
    "# Tensor datasets\n",
    "train_data_v = TensorDataset(torch.from_numpy(train_x[:800]), torch.from_numpy(train_y[:800]))\n",
    "valid_data_v = TensorDataset(torch.from_numpy(val_x[:100]), torch.from_numpy(val_y[:100]))\n",
    "test_data_v = TensorDataset(torch.from_numpy(test_x[:100]), torch.from_numpy(test_y[:100]))\n",
    "\n",
    "train_loader_v = DataLoader(train_data_v, shuffle=True, batch_size=30)\n",
    "valid_loader_v = DataLoader(valid_data_v, shuffle=True, batch_size=30)\n",
    "test_loader_v = DataLoader(test_data_v, shuffle=True, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 01:00:51.684047 4397680064 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/xianglu/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1212 01:00:51.687937 4397680064 configuration_utils.py:169] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1212 01:00:52.067970 4397680064 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/xianglu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "Bmodel = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer Bidirectional GRU Model\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim, output_dim, n_layers, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-layer \n",
    "        self.rnn = nn.GRU(768, hidden_dim, num_layers = n_layers, bidirectional = True, batch_first = True)\n",
    "\n",
    "        self.FC = nn.Linear(hidden_dim + hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        \n",
    "        out, hidden = self.rnn(Bmodel(x)[0])\n",
    "        \n",
    "        first = hidden[-1,:,:]\n",
    "        \n",
    "        # Set the dimensions for bidirectional modification\n",
    "        to_cat = torch.cat((first,first), dim = 1)\n",
    "        hidden = self.dropout(to_cat)\n",
    "                \n",
    "        output = self.FC(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "hidden_dim = [32,64,128,256,512]\n",
    "dropout = [0.1,0.15,0.2,0.25,0.3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(preds, y):\n",
    "    \n",
    "    Newpreds = torch.round(torch.sigmoid(preds))\n",
    "    d = (Newpreds == y).float()  \n",
    "    result = d.sum() / len(d)\n",
    "    \n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Dimension:  32     Dropout Rate: 0.1\n",
      "Train Loss:  0.7549700871265183      Train Accuracy: 0.27500000409781933\n",
      "Valid Loss:  0.7175967765040696      Valid Accuracy: 0.3583333417773247\n",
      "\t\n",
      "Hidden Dimension:  64     Dropout Rate: 0.1\n",
      "Train Loss:  0.6248903599878153      Train Accuracy: 0.7749999910593033\n",
      "Valid Loss:  0.6608168303966522      Valid Accuracy: 0.6916666626930237\n",
      "\t\n",
      "Hidden Dimension:  64     Dropout Rate: 0.15\n",
      "Train Loss:  0.7080828769132494      Train Accuracy: 0.45000001043081284\n",
      "Valid Loss:  0.6926709280659755      Valid Accuracy: 0.5583333447575569\n",
      "\t\n",
      "Hidden Dimension:  128     Dropout Rate: 0.15\n",
      "Train Loss:  0.6355923667550087      Train Accuracy: 0.75\n",
      "Valid Loss:  0.6508804433668653      Valid Accuracy: 0.6833333373069763\n",
      "\t\n",
      "Hidden Dimension:  128     Dropout Rate: 0.2\n",
      "Train Loss:  0.7345867486981055      Train Accuracy: 0.3500000052154064\n",
      "Valid Loss:  0.7188207088038325      Valid Accuracy: 0.3500000089406967\n",
      "\t\n",
      "Hidden Dimension:  256     Dropout Rate: 0.2\n",
      "Train Loss:  0.7003571592116108      Train Accuracy: 0.45000001043081284\n",
      "Valid Loss:  0.7063227946249148      Valid Accuracy: 0.40833334252238274\n",
      "\t\n",
      "Hidden Dimension:  256     Dropout Rate: 0.25\n",
      "Train Loss:  0.725562735646963      Train Accuracy: 0.3083333410322666\n",
      "Valid Loss:  0.7236252452091625      Valid Accuracy: 0.3500000052154064\n",
      "\t\n",
      "Hidden Dimension:  512     Dropout Rate: 0.25\n",
      "Train Loss:  0.7518695910926908      Train Accuracy: 0.3333333395421505\n",
      "Valid Loss:  0.7456994585382442      Valid Accuracy: 0.3583333380520344\n",
      "\t\n",
      "Hidden Dimension:  512     Dropout Rate: 0.3\n",
      "Train Loss:  0.6737084157764912      Train Accuracy: 0.6000000163912773\n",
      "Valid Loss:  0.6872650854755192      Valid Accuracy: 0.6000000014901161\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# Grid Search to find the best model\n",
    "k = 0\n",
    "z = 0\n",
    "for i in range(9):\n",
    "    print(\"Hidden Dimension: \",hidden_dim[k],\"    Dropout Rate:\",dropout[z])\n",
    "   \n",
    "    model = BERT_GRU(hidden_dim[k], 1, 2, dropout[z])\n",
    "    if k == z:\n",
    "        k += 1\n",
    "    else:\n",
    "        z +=1\n",
    "    tr = iter(train_loader_v)\n",
    "    te = iter(test_loader_v)\n",
    "    v = iter(valid_loader_v)\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    Tloss = 0\n",
    "    Taccuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # iterations\n",
    "    for i,j in tr:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        predictions = model(i).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, j)\n",
    "        \n",
    "        accuracy = calculate(predictions, j)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()   \n",
    "        \n",
    "        Tloss += loss.item()\n",
    "        Taccuracy += accuracy.item()\n",
    "\n",
    "    value1 = Tloss / len(tr)\n",
    "    value2 = Taccuracy / len(tr)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    Tloss = 0\n",
    "    Taccuracy = 0\n",
    "    \n",
    "    model.eval()\n",
    " \n",
    "    # iterations\n",
    "    with torch.no_grad():\n",
    "        for i,j in v:\n",
    "        \n",
    "            predictions = model(i).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, j)\n",
    "            \n",
    "            accuracy = calculate(predictions, j)\n",
    "            \n",
    "            Tloss += loss.item()\n",
    "            Taccuracy += accuracy.item()\n",
    "            \n",
    "    valid1 = Tloss / len(v)\n",
    "    valid2 = Taccuracy / len(v)       \n",
    "    \n",
    "    \n",
    "    #########################################################################################\n",
    "      \n",
    "    print(\"Train Loss: \",value1,\"     Train Accuracy:\",value2)\n",
    " \n",
    "    \n",
    "    print(\"Valid Loss: \",valid1,\"     Valid Accuracy:\",valid2)\n",
    "\n",
    "    print('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layer: 1\n",
      "Train Loss:  0.6868575837187194      Train Accuracy: 0.5641975391794134\n",
      "Valid Loss:  0.6925353186825911      Valid Accuracy: 0.5166666731238365\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# Compare to single layer model\n",
    "for i in range(1):\n",
    "    print(\"Number of Layer: 1\" )\n",
    "   \n",
    "    model = BERT_GRU( 64, 1, 1, 0.1)\n",
    "    \n",
    "    tr = iter(train_loader_v)\n",
    "    te = iter(test_loader_v)\n",
    "    v = iter(valid_loader_v)\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    Tloss = 0\n",
    "    Taccuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # iterations\n",
    "    for i,j in tr:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        predictions = model(i).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, j)\n",
    "        \n",
    "        accuracy = calculate(predictions, j)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()   \n",
    "        \n",
    "        Tloss += loss.item()\n",
    "        Taccuracy += accuracy.item()\n",
    "\n",
    "    value1 = Tloss / len(tr)\n",
    "    value2 = Taccuracy / len(tr)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    Tloss = 0\n",
    "    Taccuracy = 0\n",
    "    \n",
    "    model.eval()\n",
    " \n",
    "    # iterations\n",
    "    with torch.no_grad():\n",
    "        for i,j in v:\n",
    "        \n",
    "            predictions = model(i).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, j)\n",
    "            \n",
    "            accuracy = calculate(predictions, j)\n",
    "            \n",
    "            Tloss += loss.item()\n",
    "            Taccuracy += accuracy.item()\n",
    "            \n",
    "    valid1 = Tloss / len(v)\n",
    "    valid2 = Taccuracy / len(v)       \n",
    "    \n",
    "    \n",
    "    #########################################################################################\n",
    "      \n",
    "    print(\"Train Loss: \",value1,\"     Train Accuracy:\",value2)\n",
    " \n",
    "    \n",
    "    print(\"Valid Loss: \",valid1,\"     Valid Accuracy:\",valid2)\n",
    "\n",
    "    print('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_GRU( 64, 1, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaulation and Analysis\n",
    "v = iter(test_loader_v)\n",
    "\n",
    "total_true = 0 \n",
    "true_positive = 0 \n",
    "pred_positive = 0 \n",
    "act_positive = 0 \n",
    "false_positive = 0 \n",
    "false_negative = 0 \n",
    "\n",
    "\n",
    "for batch,batch2 in v:\n",
    "    \n",
    "\n",
    "    predictions = model(batch).squeeze(1)\n",
    "    \n",
    "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "      \n",
    "    ## total number of correct classifications\n",
    "    total_true += (rounded_preds == batch2).float().sum()\n",
    "    \n",
    "    ## correctly classify positive sentiments\n",
    "    true_positive += (rounded_preds * batch2 == 1).float().sum()\n",
    "    \n",
    "    ## all positive sentiments based on predictions\n",
    "    pred_positive += (rounded_preds == 1).float().sum()\n",
    "    \n",
    "    ## actual number of positive sentiments\n",
    "    act_positive += (batch2 == 1).float().sum()\n",
    "    \n",
    "    ## false positive: actually 0, but get 1\n",
    "    false_positive += (rounded_preds - batch2 == 1).float().sum()\n",
    "    \n",
    "    ## false negative: actually 1, but get 0 \n",
    "    false_negative += (batch2 - rounded_preds == 1).float().sum()\n",
    "\n",
    "    \n",
    "precision = true_positive/(true_positive+false_positive)\n",
    "recall = true_positive/(true_positive+false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6714)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6912)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
